# BPM_Skills

Pipeline to merge multiple BPM books (Excel), map sections to competencies, classify content by knowledge dimension and Bloom level, generate learning objectives (skills) with LLMs, and deduplicate final skills. Designed for execution in Google Colab (GPU recommended) or local Python environments.

---

## Table of Contents
- [Overview](#overview)  
- [Execution order (required)](#execution-order-required)  
- [Scripts (filenames & brief description)](#scripts-filenames--brief-description)  
- [Inputs & outputs (expected files)](#inputs--outputs-expected-files)  
- [Quick start (Colab)](#quick-start-colab)  
- [Hugging Face & LLM notes](#hugging-face--llm-notes)  
- [Configuration suggestions](#configuration-suggestions)  
- [Helpful snippets](#helpful-snippets)  
- [Improvements & caveats](#improvements--caveats)

---

## Overview
This repository contains a sequence of scripts that transform multiple BPM book table-of-contents (Excel) files into a structured set of competencies and learning objectives, using sentence-embeddings and LLMs for semantic mapping, classification, generation and deduplication.

---

## Execution order (required)
**Run the scripts in this exact order** (order matters because later steps consume outputs of previous ones):

1. **Merge books** — merge all input book spreadsheets and deduplicate sections.  
   Script: `bpm_books_merger.py`

2. **Map sections to competences** — assign each merged section (`group_name`) to competency labels.  
   Script: `sections_to_competence_mapper.py`

3. **Classify content by knowledge dimension** — infer knowledge dimension (Factual / Conceptual / Procedural / Metacognitive) for each section/content.  
   Script: `content_knowledge_dimension_classifier.py`

4. **Merge contents and classified groups** — join contents with the competence assignment (prepares consolidated dataset).  
   Script: `merge_contents_and_competences.py`

5. **Classify competencies by Bloom level** — infer Bloom taxonomy level for each competence (Lembrar, Compreender, Aplicar, Analisar, Avaliar, Criar).  
   Script: `competence_bloom_classifier.py`

6. **Generate skills / learning objectives** — use LLM + embeddings to generate several learning objectives per competence.  
   Script: `skills_generator.py`

7. **Deduplicate generated skills** — cluster & deduplicate skills generated by different models/sources ensuring minimum counts per model.  
   Script: `skills_deduplicator.py`

---

## Scripts (filenames & brief description)
- `bpm_books_merger.py`  
  Load multiple Excel book files, preprocess titles (pt/en), map sections to a chosen reference book using multilingual embeddings, validate relevance, deduplicate similar subsections, output final consolidated workbook.

- `sections_to_competence_mapper.py`  
  Read `Final_BPM_Books_Definitivo.xlsx`, compute section embeddings, and map each `group_name` to the top 1–2 competencies with similarity scores. Output: `Grupos_vs_Competencias.xlsx`.

- `content_knowledge_dimension_classifier.py`  
  Read final sections and infer the Bloom *knowledge dimension* (Factual, Conceptual, Procedural, Metacognitive) per content using an LLM. Output: `Merged_BPM_Books_Classificados.xlsx`.

- `merge_contents_and_competences.py`  
  Merge content dataset with `Grupos_vs_Competencias.xlsx` to create a unified dataset for later processing. Output: `conteudos_completo_com_competencias.xlsx`.

- `competence_bloom_classifier.py`  
  Read `conteudos_completo_com_competencias.xlsx` and infer Bloom *cognitive level* (Lembrar, Compreender, Aplicar, Analisar, Avaliar, Criar) for each unique competence (caches results). Output: `conteudos_com_bloom.xlsx`.

- `skills_generator.py`  
  Load `conteudos_com_bloom.xlsx`, select diverse supporting contents by embedding similarity, and prompt an LLM to generate an heuristic-driven number of learning objectives per competence. Output: `objetivos_de_aprendizagem_final.xlsx`.

- `skills_deduplicator.py`  
  Given an `Ementas BPM.xlsx` with sheets per competence and a "Modelo" column showing which model produced each skill, cluster skills and choose representative ones while ensuring final counts ≥ the largest per-model count. Output: `habilidades_unicas_por_competencia.xlsx`.

---

## Inputs & outputs (expected files)
- **Required inputs (examples)**:
  - `Livro_Baldam_pt.xlsx`, `Livro_Dumas_en.xlsx`, `Livro_Paim_pt.xlsx`, `Livro_Valle_Oliveira_pt.xlsx`, `Livro_Wensk_en.xlsx`
  - `Final_BPM_Books_Definitivo.xlsx`  
  - `Merged_BPM_Books_Classificados.xlsx`  
  - `Ementas BPM.xlsx`

- **Main outputs (examples)**:
  - `Final_BPM_Books_Definitivo.xlsx`  
  - `Grupos_vs_Competencias.xlsx`  
  - `Merged_BPM_Books_Classificados.xlsx`  
  - `conteudos_completo_com_competencias.xlsx`  
  - `conteudos_com_bloom.xlsx`  
  - `objetivos_de_aprendizagem_final.xlsx`  
  - `habilidades_unicas_por_competencia.xlsx`

---

## Quick start (Colab)
1. Open a new Google Colab notebook.  
2. Connect to a GPU runtime (Runtime → Change runtime type → GPU).  
3. Install requirements (or use the provided `requirements.txt`):
```bash
!pip install -r requirements.txt
```
4. Upload Excel files or mount Google Drive.
5. Download Spacy models:
```bash
!python -m spacy download pt_core_news_sm
!python -m spacy download en_core_web_sm
```
6. Run scripts in the **exact order** above.

---

## Hugging Face & LLM notes
- Scripts use models such as `meta-llama/Meta-Llama-3.1-8B-Instruct` and `mistralai/Mistral-7B-Instruct-v0.2`.
- Requires a Hugging Face token (`notebook_login()` in Colab).
- Use quantization (`bitsandbytes`) if needed to fit models in GPU memory.

---

## Configuration suggestions
- Store configurable values in a single `config.json` or via CLI args.
- Example:
```json
{
  "embedding_model": "paraphrase-multilingual-MiniLM-L12-v2",
  "llm_model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "relevance_threshold": 0.50
}
```

---

## Helpful snippets
- Save/load embeddings:
```python
np.save("section_embeddings.npy", section_embeddings)
embs = np.load("section_embeddings.npy")
```

---

## Improvements & caveats
- Validate required columns before running.
- Batch encode embeddings and cache results.
- Review generated skills for consistency and redundancy.
- Check copyright and licensing before publishing processed material.
