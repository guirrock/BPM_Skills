# -*- coding: utf-8 -*-
"""merge_sumários_classifica_competência_SBC_gera_habilidades.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E6k8eAWFS9yVu5fGcOwCMFCm2cqw5KsJ
"""

# ==============================================================================
# SCRIPT MERGE DE LIVROS DE BPM
# ==============================================================================

# --- 0. Imports ---
import pandas as pd
import spacy
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re

# --- 1. Configurações ---
book_files = [
    "Livro_Baldam_pt.xlsx",
    "Livro_Dumas_en.xlsx",
    "Livro_Paim_pt.xlsx",
    "Livro_Valle_Oliveira_pt.xlsx",
    "Livro_Wensk_en.xlsx"
]

reference_book_file = "Livro_Dumas_en.xlsx"

# --- 2. Pré-processamento ---
print("--- Iniciando Pré-processamento ---")
try:
    nlp_pt = spacy.load("pt_core_news_sm")
except OSError:
    print("Baixando modelo de português para o Spacy...")
    from spacy.cli import download
    download("pt_core_news_sm")
    nlp_pt = spacy.load("pt_core_news_sm")

try:
    nlp_en = spacy.load("en_core_web_sm")
except OSError:
    print("Baixando modelo de inglês para o Spacy...")
    from spacy.cli import download
    download("en_core_web_sm")
    nlp_en = spacy.load("en_core_web_sm")

def preprocess_text(text, lang='pt'):
    if pd.isna(text): return ""
    text = str(text).lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    nlp_model = nlp_pt if lang == 'pt' else nlp_en
    doc = nlp_model(text)
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space]
    return " ".join(tokens)

# --- 3. Carregar Livros ---
print("--- Carregando e Consolidando Livros ---")
all_sections = []
for file in book_files:
    print(f"Processando arquivo: {file}")
    df = pd.read_excel(file)
    df.columns = df.columns.str.strip()
    df['Book'] = file
    df['lang'] = 'pt' if '_pt' in file else 'en'
    df['section_id'] = df['Seção'].astype(str) + "_" + df['Capítulo'].astype(str)
    df['title_clean'] = df.apply(lambda row: preprocess_text(row['Título'], row['lang']), axis=1)
    all_sections.append(df)
df_all = pd.concat(all_sections, ignore_index=True)

# --- 4. Carregar Modelo de IA ---
print("--- Carregando Modelo de Embedding ---")
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
print("Modelo carregado.")

# --- 5. Mapeamento Inicial (Hipótese) ---
print(f"\n--- Passo 5: Mapeamento Inicial usando '{reference_book_file}' ---")
grouped_sections = df_all.groupby(['section_id', 'Capítulo'])['title_clean'].apply(lambda t: " ".join(t)).reset_index()
section_embeddings = model.encode(grouped_sections['title_clean'].tolist(), convert_to_numpy=True)
ref_ids = df_all[df_all['Book'] == reference_book_file]['section_id'].unique()
ref_indices = grouped_sections[grouped_sections['section_id'].isin(ref_ids)].index
ref_embeddings = section_embeddings[ref_indices]
ref_info = grouped_sections.loc[ref_indices].reset_index(drop=True)
sim_matrix = cosine_similarity(section_embeddings, ref_embeddings)
best_match_indices = np.argmax(sim_matrix, axis=1)
grouped_sections['section_group_id'] = ref_info['section_id'].iloc[best_match_indices].values
grouped_sections['group_name'] = ref_info['Capítulo'].iloc[best_match_indices].values
df_all = pd.merge(df_all, grouped_sections[['section_id', 'group_name', 'section_group_id']], on='section_id')
print("Mapeamento inicial concluído.")

df_all.to_excel("Intermediario_BPM_Books_Referencia_Dumas.xlsx", index=False)
print("Excel intermediário (pós-mapeamento) gerado: Intermediario_BPM_Books_Referencia_Dumas.xlsx")

# --- PASSO 6: Validação de Relevância por Similaridade de Embedding ---
print("\n--- Passo 6: Validando Relevância por Similaridade de Embedding ---")
RELEVANCE_THRESHOLD = 0.50 # Limiar para considerar uma subseção como relevante.
print(f"Usando limiar de relevância: {RELEVANCE_THRESHOLD}")

unique_group_names = df_all['group_name'].unique()
group_name_embeddings = model.encode(unique_group_names.tolist(), convert_to_numpy=True)
group_name_to_embedding = {name: emb for name, emb in zip(unique_group_names, group_name_embeddings)}
df_all['title_embedding'] = list(model.encode(df_all['title_clean'].tolist(), convert_to_numpy=True))
relevance_scores = []
is_relevant_list = []
for index, row in df_all.iterrows():
    if row['Book'] == reference_book_file:
        relevance_scores.append(1.0)
        is_relevant_list.append(True)
        continue
    group_emb = group_name_to_embedding[row['group_name']]
    title_emb = row['title_embedding']
    similarity = cosine_similarity([title_emb], [group_emb])[0][0]
    relevance_scores.append(similarity)
    is_relevant_list.append(similarity >= RELEVANCE_THRESHOLD)
df_all['score_relevancia'] = relevance_scores
df_all['is_relevant'] = is_relevant_list
print("Validação de relevância concluída.")
df_all.to_excel("Intermediario_Apos_Validacao_Relevancia.xlsx", index=False)
print("Excel intermediário (pós-validação) gerado: Intermediario_Apos_Validacao_Relevancia.xlsx")
df_relevant = df_all[df_all['is_relevant'] == True].copy()
total_rows = len(df_all)
print(f"Subseções relevantes encontradas: {len(df_relevant)} de {total_rows}")

# --- PASSO 7: Deduplicação por Similaridade de Embedding ---
print("\n--- Passo 7: Deduplicando subseções relevantes ---")
DEDUPLICATION_THRESHOLD = 0.85
final_rows = []
unique_relevant_groups = df_relevant['section_group_id'].unique()
for sec_gid in unique_relevant_groups:
    df_sec_grp = df_relevant[df_relevant['section_group_id'] == sec_gid].copy().reset_index(drop=True)
    if df_sec_grp.empty: continue
    sub_embs = np.array(df_sec_grp['title_embedding'].tolist())
    sim_matrix = cosine_similarity(sub_embs)
    clusters = []
    visited = set()
    for i in range(len(df_sec_grp)):
        if i in visited: continue
        similar_indices = np.where(sim_matrix[i] >= DEDUPLICATION_THRESHOLD)[0]
        cluster = [idx for idx in similar_indices if idx not in visited]
        visited.update(cluster)
        clusters.append(cluster)
    for cluster_indices in clusters:
        cluster_df = df_sec_grp.iloc[cluster_indices]
        livros_no_cluster = ", ".join(sorted(cluster_df['Book'].unique()))
        ref_items_in_cluster = cluster_df[cluster_df['Book'] == reference_book_file]
        if not ref_items_in_cluster.empty:
            final_title = ref_items_in_cluster.iloc[0]['Título']
        else:
            cluster_embs = sub_embs[cluster_indices]
            centroid = np.mean(cluster_embs, axis=0).reshape(1, -1)
            dists = cosine_similarity(cluster_embs, centroid)
            best_idx_in_cluster = np.argmax(dists)
            final_title = cluster_df.iloc[best_idx_in_cluster]['Título']
        for idx in cluster_indices:
            row_dict = df_sec_grp.iloc[idx].to_dict()
            row_dict['final_title'] = final_title
            status = 'Único'
            if len(cluster_indices) > 1:
                status = 'Representante' if row_dict['Título'] == final_title else 'Agrupado'
            row_dict['status_deduplicacao'] = status
            row_dict['livros_no_cluster'] = livros_no_cluster
            final_rows.append(row_dict)
df_deduplicado = pd.DataFrame(final_rows)

# --- PASSO 8: Geração do Relatório Final ---
print("\n--- Passo 8: Gerando relatório final ---")
df_final_report = df_deduplicado.copy()
colunas = [
    'group_name', 'final_title', 'status_deduplicacao',
    'original_title', 'Book', 'livros_no_cluster', 'Página',
    'score_relevancia'
]
df_final_report = df_final_report.rename(columns={'Título': 'original_title'})
colunas_existentes = [col for col in colunas if col in df_final_report.columns]
df_final_report = df_final_report.sort_values(by=['group_name', 'final_title', 'status_deduplicacao'], ascending=[True, True, False])
df_final_report[colunas_existentes].to_excel("Final_BPM_Books_Definitivo.xlsx", index=False)
print("\nProcesso concluído com sucesso!")
print("Arquivo final gerado: Final_BPM_Books_Definitivo.xlsx")