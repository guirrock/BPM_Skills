# -*- coding: utf-8 -*-
"""merge_sumários_classifica_competência_SBC_gera_habilidades.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E6k8eAWFS9yVu5fGcOwCMFCm2cqw5KsJ
"""
# ==============================================================================
# SCRIPT CLASSIFICAÇÃO CONTEÚDOS QUANTO A DIMENSÃO DO CONHECIMENTO
# ==============================================================================
# --- 1. Instalar e Importar Bibliotecas ---
print("1. Instalando bibliotecas...")
!pip install pandas openpyxl transformers accelerate bitsandbytes huggingface_hub -qqq

import pandas as pd
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import notebook_login, HfFolder, HfApi
import re
import os
from google.colab import userdata
from tqdm.notebook import tqdm

print("Bibliotecas instaladas e importadas.")

# --- 2. Autenticar no Hugging Face ---
print("\n2. Autenticando no Hugging Face...")

hf_token = None
try:
    hf_token = userdata.get('HF_TOKEN')
    os.environ["HF_TOKEN"] = hf_token
    print("Token Hugging Face carregado das Secret Keys do Colab.")
except userdata.SecretNotFoundError:
    print("Nenhuma Secret Key 'HF_TOKEN' encontrada no Colab. Verificando login interativo ou salvo.")
except Exception as e:
    print(f"Aviso: Erro ao tentar carregar 'HF_TOKEN': {e}")
    hf_token = None

if not hf_token:
    hf_token = HfFolder.get_token()
    if hf_token:
        os.environ["HF_TOKEN"] = hf_token
        print(f"Token carregado de sessão anterior (inicia com: {hf_token[:5]}...).")
    else:
        print("Nenhum token encontrado. Solicitando login interativo...")
        notebook_login()
        hf_token = HfFolder.get_token()
        os.environ["HF_TOKEN"] = hf_token

api = HfApi(token=hf_token)
api.model_info("mistralai/Mistral-7B-Instruct-v0.2")
print("Autenticação validada e acesso ao modelo confirmado.")

# --- 3. Carregar o Arquivo Excel ---
print("\n3. Carregando dados do Excel...")
file_path = 'Final_BPM_Books_Definitivo.xlsx'
sheet_name = 0  # primeira aba

if not os.path.exists(file_path):
    raise FileNotFoundError(f"Arquivo '{file_path}' não encontrado. Faça upload no Colab.")

df = pd.read_excel(file_path, sheet_name=sheet_name)
print(f"Dados carregados com sucesso! ({len(df)} linhas)")
print("Primeiras 5 linhas:")
print(df.head())

# --- 4. Carregar o Modelo ---
print("\n4. Carregando modelo Mistral 7B...")
model_id = "mistralai/Mistral-7B-Instruct-v0.2"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    token=hf_token
)

print("Modelo carregado com sucesso.")

# --- 5. Função de Classificação ---
def inferir_tipo_conhecimento(conteudo_texto, tokenizer_obj, model_obj, device):
    prompt = f"""
    Dado o seguinte conteúdo de aula, classifique-o unicamente em uma das quatro dimensões da Taxonomia de Bloom (revisada):
    - Factual
    - Conceitual
    - Procedural
    - Metacognitivo

    Conteúdo: "{conteudo_texto}"

    Classificação:
    """
    messages = [
        {"role": "system", "content": "Responda apenas com uma palavra: Factual, Conceitual, Procedural ou Metacognitivo."},
        {"role": "user", "content": prompt}
    ]

    input_ids = tokenizer_obj.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    outputs = model_obj.generate(
        input_ids,
        max_new_tokens=20,
        do_sample=False,
        temperature=0.1,
        top_p=0.9,
        eos_token_id=tokenizer_obj.eos_token_id,
        pad_token_id=tokenizer_obj.eos_token_id
    )

    full_text = tokenizer_obj.decode(outputs[0], skip_special_tokens=False)

    if "[/INST]" in full_text:
        response = full_text.split("[/INST]")[-1].strip()
    else:
        response = full_text.strip()

    response = re.sub(r"[^a-zA-ZÀ-ÿ]", " ", response).strip()
    match = re.search(r"(Factual|Conceitual|Procedural|Metacognitivo)", response, re.IGNORECASE)
    if match:
        return match.group(1).capitalize()
    return "N/A"

# --- 6. Classificar e Salvar ---
print("\n6. Classificando conteúdos...")
model_device = model.device

classificacoes = []
for idx, row in tqdm(df.iterrows(), total=len(df), desc="Classificando"):
    conteudo = str(row['final_title'])
    if not conteudo or pd.isna(conteudo):
        classificacoes.append("N/A")
        continue
    try:
        classificacoes.append(inferir_tipo_conhecimento(conteudo, tokenizer, model, model_device))
    except Exception as e:
        classificacoes.append("N/A")

df['Tipo_Conhecimento_Inferido'] = classificacoes

# --- 7. Exportar ---
output_file = 'Merged_BPM_Books_Classificados.xlsx'
df.to_excel(output_file, index=False)
print(f"\nProcesso concluído! Resultados salvos em '{output_file}'.")

