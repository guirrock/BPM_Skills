# -*- coding: utf-8 -*-
"""merge_sumÃ¡rios_classifica_competÃªncia_SBC_gera_habilidades.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E6k8eAWFS9yVu5fGcOwCMFCm2cqw5KsJ
"""
# ==================================================================
# SCRIPT DEDUPLICAÃ‡ÃƒO DAS HABILIDADES GERADAS POR DIFERENTES MODELOS
# Garante que o nÃºmero final â‰¥ nÃºmero inicial por modelo
# Testa thresholds de 0.58 a 0.70 (passo de 0.02)
# ==================================================================

!pip install -q sentence-transformers openpyxl

from google.colab import files
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

print("FaÃ§a upload do arquivo 'Ementas BPM.xlsx'")
uploaded = files.upload()
filename = list(uploaded.keys())[0]

# ===========================================
# 1. Planilhas de interesse
# ===========================================
sheet_names = [
    "C1 - Modelar processos de negÃ³c",
    "C2 - Analisar processos de negÃ³",
    "C3 - Projetar processos de negÃ³",
    "C4 - Implementar processos de n"
]

# ===========================================
# 2. Modelo de embeddings
# ===========================================
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# ===========================================
# 3. FunÃ§Ã£o principal para processar cada planilha
# ===========================================
def processar_planilha(sheet_name, df):
    print(f"\nðŸ”¹ Processando {sheet_name} ({len(df)} habilidades)")

    df = df.dropna(subset=["Habilidade"]).reset_index(drop=True)
    if df.empty:
        print(f"Nenhuma habilidade vÃ¡lida em {sheet_name}.")
        return pd.DataFrame()

    # Calcula o nÃºmero mÃ­nimo permitido (mÃ¡ximo por modelo)
    qtd_por_modelo = df["Modelo"].value_counts().to_dict()
    limite_minimo = max(qtd_por_modelo.values())
    print(f"Cada modelo gerou entre {min(qtd_por_modelo.values())} e {limite_minimo} habilidades.")

    # Embeddings
    print("Gerando embeddings...")
    embeddings = model.encode(df["Habilidade"].tolist(), show_progress_bar=False)

    # Testa thresholds de 0.58 a 0.70 com passo 0.02
    thresholds = np.arange(0.58, 0.72, 0.02)
    for threshold in thresholds:
        clustering = AgglomerativeClustering(
            metric='cosine',
            linkage='average',
            distance_threshold=1 - threshold,
            n_clusters=None
        )
        clustering.fit(embeddings)
        df["Cluster"] = clustering.labels_
        num_clusters = len(df["Cluster"].unique())

        if num_clusters >= limite_minimo:
            print(f"Threshold definido em {threshold:.2f} â†’ {num_clusters} clusters (â‰¥ {limite_minimo})")
            break
    else:
        print(f"Mesmo no threshold {thresholds[-1]:.2f}, obtidos apenas {num_clusters} clusters (< {limite_minimo})")

    # Similaridades
    similarity_matrix = cosine_similarity(embeddings)

    cluster_data = []
    for cluster_id in sorted(df["Cluster"].unique()):
        cluster_subset = df[df["Cluster"] == cluster_id]
        indices = cluster_subset.index.tolist()

        sims = similarity_matrix[np.ix_(indices, indices)]
        mean_sim = sims.mean(axis=1)
        rep_idx = indices[np.argmax(mean_sim)]
        habilidade_rep = df.loc[rep_idx, "Habilidade"]

        modelos = cluster_subset["Modelo"].unique().tolist()
        qtd_modelos = len(modelos)

        cluster_data.append({
            "Cluster": cluster_id,
            "Habilidade_Representativa": habilidade_rep,
            "Qtd_Habilidades": len(cluster_subset),
            "Modelos_IncluÃ­dos": ", ".join(modelos),
            "Qtd_Modelos": qtd_modelos,
            "Habilidades_Agrupadas": " | ".join(cluster_subset["Habilidade"].tolist())
        })

    result_df = pd.DataFrame(cluster_data)
    print(f"{sheet_name}: {len(result_df)} habilidades Ãºnicas finais (limite mÃ­nimo: {limite_minimo}).")
    return result_df

# ================================================
# 4. Executa o processamento para cada competÃªncia
# ================================================
resultados = {}
for sheet_name in sheet_names:
    try:
        df_sheet = pd.read_excel(filename, sheet_name=sheet_name)
        expected_cols = ["CÃ³digo", "Habilidade", "Modelo", "Link", "Data"]
        missing = [c for c in expected_cols if c not in df_sheet.columns]
        if missing:
            print(f"Colunas ausentes em {sheet_name}: {missing}")
            continue

        result_df = processar_planilha(sheet_name, df_sheet)
        if not result_df.empty:
            resultados[sheet_name] = result_df

    except Exception as e:
        print(f"Erro ao processar {sheet_name}: {e}")

# ===========================================
# 5. Exporta todas as competÃªncias para um Ãºnico Excel
# ===========================================
output_file = "habilidades_unicas_por_competencia.xlsx"
with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
    for sheet, data in resultados.items():
        sheet_name_limited = sheet[:30]
        data.to_excel(writer, sheet_name=sheet_name_limited, index=False)

print("\nArquivo final gerado com mÃºltiplas abas (uma por competÃªncia).")